{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVjkJdwgnHvM",
        "outputId": "dbe5742c-ba0f-4699-e0e7-46c9a3e2d321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wIePu2wBzmJ6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import math\n",
        "import re\n",
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning up the documents"
      ],
      "metadata": {
        "id": "3La2Hn-3w7FM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kf1P344qvXXT"
      },
      "outputs": [],
      "source": [
        "def separate_doc(path):\n",
        "    with open(path, 'r') as doc_collection:\n",
        "        doc = doc_collection.read().split(\".I \")\n",
        "        for i in range(1, len(doc)):\n",
        "            doc[i] = doc[i].split(\"\\n.S\", maxsplit = 1)\n",
        "    return doc\n",
        "\n",
        "def doc_preprocess(txt):\n",
        "  txt = re.sub('[\\W_]+', ' ', txt.lower()) # substitutes non-word characters and _ with spaces\n",
        "  txt = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", txt) # substitutes digits in the beginning, middle, and end of words\n",
        "  txt = re.sub(r'[\\W_]+', ' ', txt) # rechecking for non-word characters\n",
        "  txt = txt.split()\n",
        "  return txt\n",
        "\n",
        "def remove_stopwords(txt, stopwords):\n",
        "  # word = txt.split()\n",
        "  filtered_words = [word for word in txt.split() if word not in stopwords] # removes all the stopwords\n",
        "  # filtered_words = list(filter(lambda x: x not in stopwords, word)) \n",
        "  txt = ' '.join(filtered_words)\n",
        "  return txt\n",
        "\n",
        "def id_and_content(docs, stopwords):\n",
        "  clean_docs = {}\n",
        "  length = len(docs)\n",
        "  # print(length)\n",
        "  for i in range(1, length - 1):\n",
        "    try:\n",
        "      id = docs[i][0][9:]\n",
        "      txt = docs[i][1]\n",
        "    except:\n",
        "      continue\n",
        "    txt = remove_stopwords(txt, stopwords)\n",
        "    txt = doc_preprocess(txt)\n",
        "    clean_docs[int(id)] = txt\n",
        "  return clean_docs # returns the docs after all the preprocessing has been done and separates then into id and content dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the index and inverted index"
      ],
      "metadata": {
        "id": "uWEQ1XaaxBCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-e6-tRDAzWvL"
      },
      "outputs": [],
      "source": [
        "def file_index(list_of_words):\n",
        "  all_docs = {}\n",
        "  for files, words in list_of_words.items():\n",
        "    index = {}\n",
        "    for i, word in enumerate(words):\n",
        "      if word in index.keys():\n",
        "        index[word].append(i)\n",
        "      else:\n",
        "        index[word] = [i]\n",
        "    all_docs[files] = index\n",
        "  return all_docs\n",
        "\n",
        "def complete_inverted_index(indexx):\n",
        "  full_inv_index = defaultdict(dict)\n",
        "  for files, file_idx in indexx.items():\n",
        "    for word, indices in file_idx.items():\n",
        "      full_inv_index[word][files] = indices\n",
        "  return dict(full_inv_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing through and cleaning up the queries"
      ],
      "metadata": {
        "id": "Xrel65A4xEKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_queries(files):\n",
        "  queries = []\n",
        "  with open(files, 'r') as f:\n",
        "    current_query = None\n",
        "    for line in f:\n",
        "      line = line[:-1]\n",
        "      if '<top>' in line:\n",
        "        current_query = {}\n",
        "      elif '</top>' in line:\n",
        "        queries.append(current_query)\n",
        "        current_query = {}\n",
        "      elif '<num>' in line:\n",
        "        current_query['num'] = line.split(':')[1].strip()\n",
        "      elif '<title>' in line:\n",
        "        current_query['title'] = line.split('>')[1].strip()\n",
        "      elif (not '<desc>' in line and len(line) > 2):\n",
        "        current_query['description'] = line\n",
        "  queries_all = {}\n",
        "  length_queries = len(queries)\n",
        "  for i in range(length_queries):\n",
        "    queries_all[queries[i]['num']] = queries[i]['description']\n",
        "  return queries_all\n",
        "\n",
        "def preprocessing_queries(query, stopwords):\n",
        "  for i, j in query.items():\n",
        "    txt = query[i]\n",
        "    txt = remove_stopwords(txt, stopwords)\n",
        "    txt = doc_preprocess(txt)\n",
        "    query[i] = txt\n",
        "  return query "
      ],
      "metadata": {
        "id": "ZqQ6ASNxL5ZI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_of_dq(doc, query):\n",
        "  intersection = set(list(index[doc].keys())) & set(query)\n",
        "  score = len(list(intersection))\n",
        "  return score\n",
        "\n",
        "def scores_top_50(doc_score):\n",
        "  doc_score_sorted = sorted(list(doc_score.items()), key = lambda item: item[1], reverse = True)\n",
        "  return doc_score_sorted[:50]"
      ],
      "metadata": {
        "id": "ef_P_Y_GuutT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ranking algorithms"
      ],
      "metadata": {
        "id": "2Hn9bIWMxHzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def boolean_ranking(docs, query):\n",
        "  documents_score = {}\n",
        "  for doc in docs:\n",
        "    boolean_score = intersection_of_dq(doc, query)\n",
        "    documents_score[doc] = boolean_score\n",
        "  boolean_50 = scores_top_50(documents_score)\n",
        "  return boolean_50\n",
        "\n",
        "def tf_ranking(docs, query):\n",
        "  documents_score = {}\n",
        "  for doc in docs:\n",
        "    len_document = len(index[doc])\n",
        "    frequency = 0\n",
        "    for term in query:\n",
        "      term_freq = 0\n",
        "      if term in index[doc]:\n",
        "        term_freq = len(index[doc][term])\n",
        "      frequency += term_freq\n",
        "    actual_freq = frequency / len_document\n",
        "    documents_score[doc] = actual_freq\n",
        "  tf_50 = scores_top_50(documents_score)\n",
        "  return tf_50\n",
        "\n",
        "def tf_idf_ranking(docs, query):\n",
        "  documents_score = {}\n",
        "  for doc in docs:\n",
        "    len_document = len(index[doc])\n",
        "    frequency = 0\n",
        "    for term in query:\n",
        "      term_freq = 0\n",
        "      if term in index[doc]:\n",
        "        term_freq = len(index[doc][term])\n",
        "      frequency += term_freq\n",
        "    actual_freq = frequency / len_document\n",
        "    inverted_doc_freq = 0\n",
        "    for term in query:\n",
        "      if term in inverted_index:\n",
        "        doc_freq = len(inverted_index[term])\n",
        "        idf = len(inverted_index) / doc_freq\n",
        "        idf = math.log(idf)\n",
        "        inverted_doc_freq +=idf\n",
        "    tf_idf = actual_freq * inverted_doc_freq\n",
        "    documents_score[doc] = tf_idf\n",
        "  tf_idf_50 = scores_top_50(documents_score)\n",
        "  return tf_idf_50\n",
        "\n",
        "def custom_ranking(docs, query):\n",
        "  documents_score = tf_idf_ranking(docs, query)\n",
        "  documents = [i for i, j in documents_score]\n",
        "  doc_score = {}\n",
        "  for doc in documents:\n",
        "    len_document = len(index[doc])\n",
        "    frequency = 0\n",
        "    for term in query:\n",
        "      term_freq = 0\n",
        "      if term in index[doc]:\n",
        "        term_freq = len(index[doc][term])\n",
        "      frequency += term_freq\n",
        "    actual_freq = frequency / len_document\n",
        "    inverted_doc_freq = 0\n",
        "    for term in query:\n",
        "      if term in inverted_index:\n",
        "        doc_freq = len(inverted_index[term])\n",
        "        idf = len(inverted_index) / doc_freq\n",
        "        idf = math.log(idf)\n",
        "        inverted_doc_freq +=idf\n",
        "    tf_idf = actual_freq * inverted_doc_freq\n",
        "    new_score = (tf_idf * 2) - math.log(tf_idf)\n",
        "    doc_score[doc] = new_score\n",
        "  custom_50 = scores_top_50(doc_score)\n",
        "  return custom_50"
      ],
      "metadata": {
        "id": "cyMfJtuCUCy8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "XkXGMHJzxL7x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNC4fUS159dS"
      },
      "outputs": [],
      "source": [
        "# extending the set of stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.extend(['.U', '.S','.M','.T','.P','.W','.M','.I'])\n",
        "\n",
        "# getting preprocessing the docs and creating the index and inverted index\n",
        "docs = separate_doc('/content/drive/MyDrive/CSE 272/HW 1_Search Engine/ohsumed.88-91')\n",
        "# print(docs)\n",
        "cleaned_docs = id_and_content(docs, stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_index = file_index(cleaned_docs)\n",
        "full_inverted_index = complete_inverted_index(full_index)"
      ],
      "metadata": {
        "id": "8YEyIgtR0JhD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pickles to save the index in binary format\n",
        "index_binary = open(\"/content/drive/MyDrive/CSE 272/HW 1_Search Engine/index.pkl\", \"wb\")\n",
        "pickle.dump(full_index, index_binary)\n",
        "index_binary.close()"
      ],
      "metadata": {
        "id": "_mDenfG4kf6T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FaJ-3o7n9880"
      },
      "outputs": [],
      "source": [
        "# Using pickles to save the inverted index in binary format\n",
        "inverted_index_binary = open(\"/content/drive/MyDrive/CSE 272/HW 1_Search Engine/inverted_index.pkl\", \"wb\")\n",
        "pickle.dump(full_inverted_index, inverted_index_binary)\n",
        "inverted_index_binary.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "deEw_nWaWPRA"
      },
      "outputs": [],
      "source": [
        "index_file = open(\"/content/drive/MyDrive/CSE 272/HW 1_Search Engine/index.pkl\", \"rb\")\n",
        "index = pickle.load(index_file)\n",
        "\n",
        "inv_index_file = open(\"/content/drive/MyDrive/CSE 272/HW 1_Search Engine/inverted_index.pkl\", \"rb\")\n",
        "inverted_index = pickle.load(inv_index_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing through the queries and preprocessing them\n",
        "queries = parse_queries('/content/drive/MyDrive/CSE 272/HW 1_Search Engine/query.ohsu.1-63')\n",
        "all_queries = preprocessing_queries(queries, stopwords)"
      ],
      "metadata": {
        "id": "CL4dGBpz4-Vv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the log files for each ranking algorithm"
      ],
      "metadata": {
        "id": "PSYtmj4uxOwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def FTQ(query):\n",
        "  docs = set()\n",
        "  for t in query:\n",
        "    if t in inverted_index:\n",
        "      docs.update(set(inverted_index[t].keys()))\n",
        "  return list(docs)\n",
        "\n",
        "def print_ranking_algos(algorithm):\n",
        "  ranking_function = {\n",
        "      'Boolean': boolean_ranking,\n",
        "      'Tf': tf_ranking,\n",
        "      'Tf_idf': tf_idf_ranking,\n",
        "      'Custom':custom_ranking,\n",
        "  }\n",
        "  with open(algorithm, 'w') as f:\n",
        "    for q_id, query in queries.items():\n",
        "      docs = FTQ(query)\n",
        "      docs_score = ranking_function[algorithm](docs, query)\n",
        "      for i, (doc_id, score) in enumerate(docs_score):\n",
        "        f.write(f\"{q_id}\\tQ0\\t{doc_id}\\t\\t{i+1}\\t{score}\\t{algorithm}\\n\")"
      ],
      "metadata": {
        "id": "6eT386yjeMsZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_ranking_algos('Boolean')\n",
        "print_ranking_algos('Tf')\n",
        "print_ranking_algos('Tf_idf')\n",
        "print_ranking_algos('Custom')"
      ],
      "metadata": {
        "id": "p-xKx_FSkd6v"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}